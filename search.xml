<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[scrapy_redis-docker-crawler]]></title>
    <url>%2F2017%2F08%2F22%2Fscrapy-redis-docker-crawler%2F</url>
    <content type="text"><![CDATA[scrapy_redis 和 docker 实现简单分布式爬虫简介（以前写的一篇，搬过来） 在使用 scrapy 爬取 IT桔子公司信息，用来进行分析，了解 IT 创业公司的一切情况，之前使用 scrapy 写了一个默认线程是10的单个实例，为了防止被 ban IP 设置了下载的速度，3万多个公司信息爬了1天多才完成，现在想到使用分布式爬虫来提高效率。 源码githup ####技术工具：Python3.5 scrapy scrapy_redis redis docker1.12 docker-compose Kitematic mysql SQLAlchemy 准备工作 安装 Docker 点这里去了解、安装; pip install scrapy scrapy_redis; 代码编写 分析页面信息：我需要获取的是每一个「公司」的详情页面链接 和 分页按钮链接； 统一存储获取到的链接，提供给多个 spider 爬取； 多个 spider 共享一个 redis list 中的链接； ###目录结构图 ###juzi_spider.py1234567891011121314151617181920212223242526272829# coding:utf-8from bs4 import BeautifulSoupfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom scrapy_redis.spiders import RedisCrawlSpiderfrom itjuzi_dis.items import CompanyItemclass ITjuziSpider(RedisCrawlSpider): name = 'itjuzi_dis' allowed_domains = ['itjuzi.com'] # start_urls = ['http://www.itjuzi.com/company/157'] redis_key = 'itjuziCrawler:start_urls' rules = [ # 获取每一页的链接 Rule(link_extractor=LinkExtractor(allow=('/company\?page=\d+'))), # 获取每一个公司的详情 Rule(link_extractor=LinkExtractor(allow=('/company/\d+')), callback='parse_item') ] def parse_item(self, response): soup = BeautifulSoup(response.body, 'lxml') . .省略一些处理代码 . return item 说明： class 继承了RedisCrawlSpider 而不是CrawlSpider start_urls 改为一个自定义的 itjuziCrawler:start_urls,这里的itjuziCrawler:start_urls 就是作为所有链接存储到 redis 中的 key,scrapy_redis 里也是通过redis的 lpop方法弹出并删除链接的； ###db_util.py使用 SQLAlchemy 作为 ORM 工具，当表结构不存在时，自动创建表结构 ###middlewares.py增加了很多 User-Agent，每一个请求随机使用一个，防止防止网站通过 User-Agent 屏蔽爬虫 ###settings.py配置middlewares.py scrapy_redis redis 链接相关信息 ##部署在上面的「目录结构图」中有，Dockerfile和docker-compose.yml Dockerfile1234567FROM python:3.5ENV PATH /usr/local/bin:$PATHADD . /codeWORKDIR /codeRUN pip install -r requirements.txtCOPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redisCMD /usr/local/bin/scrapy crawl itjuzi_dis 说明： 使用 python3.5作为基础镜像 将/usr/local/bin设置环境变量 映射 host 和 container 的目录 安装 requirements.txt 特别要说明的是COPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redis，将 host 中的 spiders.py 拷贝到container 中的 scrapy_redis 安装目录中，因为 lpop 获取redis 的值在 python2中是 str 类型，而在 python3中是 bytes 类型，这个问题在 scrapy_reids 中需要修复，spiders.py 第84行需要修改； 启动后立即执行爬行命令 scrapy crawl itjuzi_dis docker-compose.yml1234567891011121314version: '2'services: spider: build: . volumes: - .:/code links: - redis depends_on: - redis redis: image: redis ports: - "6379:6379" 说明: 使用第2版本的 compose 描述语言 定义了 spider 和 redis 两个 service spider默认使用当前目录的 Dockerfile 来创建，redis使用 redis:latest 镜像创建，并都映射6379端口 ###开始部署 启动 container docker-compose up #从 docker-compose.yml 中创建 container 们 docker-compose scale spider=4 #将 spider 这一个服务扩展到4个，还是同一个 redis 可以在 Kitematic GUI 工具中观察创建和运行情况； 在没有设置 start_urls 时，4个 container 中的爬虫都处于饥渴的等待状态 现在给 redis 中放入 start_urls: lpush itjuziCrawler:start_urls http://www.itjuzi.com/company 4个爬虫都动起来了，一直爬到start_urls为空 以上です！ありがとうございました！]]></content>
      <tags>
        <tag>docker</tag>
        <tag>Python</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-algorithm-day-by-day]]></title>
    <url>%2F2017%2F08%2F18%2Fpython-algorithm-day-by-day%2F</url>
    <content type="text"><![CDATA[Python 算法题 找出100w 个数字中的最大的前100 12345import heapqimport randomdef top100(): data_list = [random.randint(0, 1000000) for x in range(1000000)] # 随机产生100w个数据 return heapq.nlargest(100, data_list)]]></content>
      <tags>
        <tag>algorithm</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[what-is-super-in-python]]></title>
    <url>%2F2017%2F08%2F16%2Fwhat-is-super-in-python%2F</url>
    <content type="text"><![CDATA[理解 Python 中的 superWhy之前经常在代码中按照套路的使用super ，但是并没有很好的去理解super 的到底是什么，阅读了大神的文章加上自己的理解，准备记录一下。 https://rhettinger.wordpress.com/2011/05/26/super-considered-super/ 特点 super 可以实现无需指定 superclass 的方法名调用 superclass 的方法 只适合新式类 MRO1class A: 从字面理解super() 就是用来调用 superclass 的，但是 Python 是可以多继承的，在有多个superclass 的时候，怎么确定super().say()]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySql常用优化方法]]></title>
    <url>%2F2017%2F08%2F16%2Fmysql-optimize%2F</url>
    <content type="text"><![CDATA[##MySql慢查日志的开启 -开启慢查日志记录show variables like ‘slow_query_log’;set global slow_query_log = on;-打开 记录没使用索引的查询set global log_queries_not_using_indexes = on;-设置长查询的时长为 1 秒set global long_query_time=1; ##查看慢查日志格式查看 long_query_log_file 的文件格式 如下： -执行时间 # Time: 160601 21:23:21 -登录的用户和 IP地址 # User@Host: root[root] @ localhost [127.0.0.1] Id: 84 -Query_time:查询所耗时间 Lock_time:锁表时间 Rows_send:发送的行数 Row_excmined:扫描的行数 # Query_time: 0.006237 Lock_time: 0.003211 Rows_sent: 11 Rows_examined: 11 -时间戳形式记录的执行时间 SET timestamp=1464787401; -具体执行的 SQL select * from t_wjjl_jbxx; ##慢日志分析工具 ###工具1：mysqldumpslowMySQL 官方自带的工具，在 mysql 安装目录的 bin 文件夹下执行命令 mysqldumpslow -t 2 /usr/local/mysql/data/Hardy-Cao-MacBook-Pro-slow.log获得如下输出： Reading mysql slow query log from /usr/local/mysql/data/Hardy-Cao-MacBook-Pro-slow.log Count: 1 Time=0.00s (0s) Lock=0.00s (0s) Rows=11.0 (11), root[root]@localhost select * from t_wjjl_jbxx Count: 1 Time=0.00s (0s) Lock=0.00s (0s) Rows=1.0 (1), [Hardy]@localhost select @@version_comment limit N 其中包括 SQL 执行的次数、所耗时间、锁定时间、反馈行数、用户 IP 信息和 SQL 具体内容。可以使用 mysqldumpslow -h 查看其他命令 ###工具2：pt-query-digest首先安装 pt-query-digest:sudo brew install percona-toolkit在 MAC 下可能出现错误：Cowardly refusing to ‘sudo brew install’,参考如下链接http://www.voidcn.com/blog/jiaolongdy/article/p-5778849.html继续，使用 pt-query-digest --help 获取帮助文档常用的是 –limit 获取一部分 –review 将日志存储到数据库表里执行命令：pt-query-digest /usr/local/mysql/data/Hardy-Cao-MacBook-Pro-slow.log |more 第一部分 # 230ms user time, 50ms system time, 28.41M rss, 2.36G vsz # Current date: Wed Jun 1 23:08:39 2016 # Hostname: Hardy-Cao-MacBook-Pro.local # Files: /usr/local/mysql/data/Hardy-Cao-MacBook-Pro-slow.log # Overall: 22 total, 6 unique, 0.01 QPS, 0.00x concurrency _______________ # Time range: 2016-06-01 21:23:21 to 22:00:06 # Attribute total min max avg 95% stddev median # ============ ======= ======= ======= ======= ======= ======= ======= # Exec time 8ms 1us 6ms 366us 60us 1ms 5us # Lock time 3ms 0 3ms 145us 0 653us 0 # Rows sent 12 0 11 0.55 0 2.26 0 # Rows examine 11 0 11 0.50 0 2.26 0 # Query size 541 12 32 24.59 26.08 5.33 26.08 第二部分 # Profile # Rank Query ID Response time Calls R/Call V/M Item # ==== ================== ============= ===== ====== ===== =============== # 1 0x07E2FD76BAFC9A3D 0.0062 77.3% 1 0.0062 0.00 SELECT t_wjjl_jbxx # 2 0xE3A3649C5FAC418D 0.0015 19.1% 1 0.0015 0.00 SELECT # MISC 0xMISC 0.0003 3.5% 20 0.0000 0.0 &lt;4 ITEMS&gt; 第三部分 # Query 1: 0 QPS, 0x concurrency, ID 0x07E2FD76BAFC9A3D at byte 0 ________ # This item is included in the report because it matches --limit. # Scores: V/M = 0.00 # Time range: all events occurred at 2016-06-01 21:23:21 # Attribute pct total min max avg 95% stddev median # ============ === ======= ======= ======= ======= ======= ======= ======= # Count 4 1 # Exec time 77 6ms 6ms 6ms 6ms 6ms 0 6ms # Lock time 100 3ms 3ms 3ms 3ms 3ms 0 3ms # Rows sent 91 11 11 11 11 11 0 11 # Rows examine 100 11 11 11 11 11 0 11 # Query size 4 25 25 25 25 25 0 25 # String: # Databases kaoqin # Hosts localhost # Users root # Query_time distribution # 1us # 10us # 100us # 1ms ################################################################ # 10ms # 100ms # 1s # 10s+ # Tables # SHOW TABLE STATUS FROM `kaoqin` LIKE &apos;t_wjjl_jbxx&apos;\G # SHOW CREATE TABLE `kaoqin`.`t_wjjl_jbxx`\G # EXPLAIN /*!50100 PARTITIONS*/ 省略了部分 Query select from t_wjjl_jbxx\G第一部分：其中一共22个 sql 查询，唯一的有6个，Time-Range 是 sql 的时间范围，第一列是执行、锁表、发送行、扫描行和查询结果大小等，其中要注意的列是「95%」是指95%的记录的值。第二部分：是哪些具体的 SQL 消耗的性能最多，包括反馈时间、占用的时间比、执行次数和具体SQL(Item)。*第三部分：是一个具体的 SQL 的性能分析，和第一部分类似，多出了这个 SQL 具体执行的数据库和主机等信息。 ##通过慢查日志找出问题 SQL 查询次数多且查询时间长的 SQL.一般为pt-query-digest分析结果的前几项查询，如上「第三部分」的 Query1，后面省略了其他 Query。 I/O比较大的 SQL注意pt-query-digest分析中 Rows examine（扫描行） 项。 未命中索引的 SQL注意pt-query-digest分析中 Rows sent (发送行) 和 Rows examine (扫描行) 的比较分析,如果 Row examine 远远大于 Rows sent，说明索引的命中率很低，这个 SQL 就需要重点关注。##具体分析问题查询 SQL使用explain从句：explain select * from t_wjjl_jbxx,得到如下结果： +—-+————-+————-+——+—————+——+———+——+——+——-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+————-+——+—————+——+———+——+——+——-+ | 1 | SIMPLE | t_wjjl_jbxx | ALL | NULL | NULL | NULL | NULL | 10 | NULL | +—-+————-+————-+——+—————+——+———+——+——+——-+ 1 rows in set (0.01 sec)table：显示这行数据是关于哪个表；type：显示连接使用了何种类型，从最好到最差为：const、eq_reg、ref、range、index 和 all，具体含义自行 google；possible_keys：显示可能应用到这张表中的索引，如果为空，没可用的索引；key：实际使用的索引，为空表示没使用索引；key_len：索引长度，在不损失精度的情况下，越短越好；ref：显示哪一列索引被使用了，情况允许的话，是一个常数最好；rows：Mysql认为必须用来检查返回请求数据的行数，也就是扫描的行数；extra：扩展列，如果出现了using filesort 和 using temporary 说明需要优化了 using filesort：说明 mysql 需要使用其他资源对所有的结果进行(order by)排序; using temporary：说明 mysql 需要创建临时表空间来存储结果，通常问题出现在 order by 上，而不是 group by 上； ##优化 MAX() 和 Count() ###MAX()执行 explain select max(c_jssj) from t_wjjl_lc \G 如下： * 1. row * id : 1 select_type : SIMPLE table : t_wjjl_lc type : ALL possible_keys: NULL key : NULL key_len : NULL ref : NULL rows : 51 Extra : NULL 1 rows in set (0.01 sec)结果是全表扫描，这里可以在 c_jssj 上创建索引优化性能，因为索引是顺序排列的，所以很快能找到最大的结果；创建索引 create index idx_jssj on t_wjjl_lc(c_jssj);后再分析，如下： * 1. row * id : 1 select_type : SIMPLE table : NULL type : NULL possible_keys: NULL key : NULL key_len : NULL ref : NULL rows : NULL Extra : Select tables optimized away 1 rows in set (0.01 sec)rows 项明显变化，说明没有全表扫描，Select tables optimized away 也表示不能再优化了； ###COUNT()MySQL 中COUNT()不等于 count(某列)，COUNT 是忽略 NULL 的；一个 COUNT 优化的例句：列出 TABLE1 count；SELECT COUNT(YEAR=2010 OR NULL),COUNT(YEAR=2011 OR NULL) FROM TABLE1 ;利用 NULL 被 COUNT 忽略的特性。 ##子查询的优化子查询的优化： 将子查询转化成 join 链接的方式； 如果子查询存在重复数据，直接 join 的话结果肯定存在重复记录，使用 distinct 去重复；##GROUP BY 优化原 SQL: SELECT actor.first_name,actor.last_name,COUNT() FROM film_actor INNER JOIN actor USING(actor_id) GROUP BY film_actor.actor_id;修改后： SELECT actor.first_name,actor.last_name,c.cnt FROM actor INNER JOIN( SELECT actor_id,COUNT() AS cnt FROM actor_film GROUP BY actor_id) AS c USING(actor_id); ##LIMIT 优化limit 常用于分页处理，时常伴随 order by 从句使用，因此大多时间会使用 filesorts 这样会造成大量的 IO 问题。 explain select c_file_id,c_blbmmc from t_wjjl_lc order by c_blbmmc limit 20,5; +—-+————-+———–+——+—————+——+———+——+——+—————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+———–+——+—————+——+———+——+——+—————-+ | 1 | SIMPLE | t_wjjl_lc | ALL | NULL | NULL | NULL | NULL | 51 | Using filesort | +—-+————-+———–+——+—————+——+———+——+——+—————-+ 1 rows in set (0.01 sec) 上面结果中的 Extra 中显示使用了 filesort，数据量大的时候对 IO 产生影响，如果表中主键是连续递增的，或者新建一个字段（index_id）来保证连续递增，可以在这个字段上创建索引，使用如下优化方式： 使用索引的列或者主键进行 Order by 操作； explain select c_file_id,c_blbmmc from t_wjjl_lc order by c_id limit 20,5; +—-+————-+———–+——-+—————+———+———+——+——+——-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+———–+——-+—————+———+———+——+——+——-+ | 1 | SIMPLE | t_wjjl_lc | index | NULL | PRIMARY | 4 | NULL | 25 | NULL | +—-+————-+———–+——-+—————+———+———+——+——+——-+ 1 rows in set (0.00 sec) 使用主键 c_id 排序后， 不使用 filesort 了，但是 rows 还是25，如果是查询500后面的5条记录，rows 就是505，越往后越多，性能也需要优化； 记录上一次返回的主键（或者 index_id），在下次查询时使用主键过滤； explain select c_id, c_file_id,c_blbmmc from t_wjjl_lcwhere c_id &gt;20762 and c_id&lt;=20767 order by c_id limit 0,5; +—-+————-+———–+——-+—————+———+———+——+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+———–+——-+—————+———+———+——+——+————-+ | 1 | SIMPLE | t_wjjl_lc | range | PRIMARY | PRIMARY | 4 | NULL | 5 | Using where | +—-+————-+———–+——-+—————+———+———+——+——+————-+ 1 rows in set (0.01 sec) 这样可以实现 rows 一直只有5，而且也不会有很大的 IO 操作； ##索引优化 ###如何合适的建立索引？ 通常情况下选择在 where、group by、order by、on 中出现的字段上建立索引； 特殊情况下可以在 select 语句的字段上建立覆盖索引(包含 select 语句全部字段)，一般是这个 select 语句的执行频率很高而且包含的字段比较少； 索引字段越小越好； 建立联合索引的时候，离散程度越高的字段放到联合索引的前面。通过对字段的 count（distinct column），值越大说明离散程度越高；###通过索引优化 SQL 的方法重复索引：相同的表中相同的列以相同的顺序建立的同类索引。允许索引：多个索引的前缀列相同，或者是联合索引中包含了主键列的索引。建立重复索引可能会减少查询时间，但是会增加修改、删除、插入的时间，所以要删除重复的索引。 使用 sql 查询出重复索引use information_schema; SELECTa.TABLE_SCHEMA AS &apos;数据名&apos;, a.table_name AS &apos;表名&apos;, a.index_name AS &apos;索引1&apos;, b.index_name AS &apos;索引2&apos;, a.column_name AS &apos;重复列名&apos; FROM STATISTICS a JOIN STATISTICS b ON a.TABLE_SCHEMA = b.TABLE_SCHEMA AND a.TABLE_NAME = b.table_name AND a.SEQ_IN_INDEX = b.SEQ_IN_INDEX AND a.COLUMN_NAME = b.COLUMN_NAME WHERE a.SEQ_IN_INDEX = 1 AND A.INDEX_NAME &lt;&gt; b.INDEX_NAME 使用 pt-duplicate-key-checker 工具在 Macbook Pro 使用出现如下错误： install_driver(mysql) failed: Attempt to reload DBD/mysql.pm aborted. Compilation failed in require at (eval 21) line 3. 在 google 后找到解决方法：ln -s /usr/local/mysql-5.6.24-osx10.8-x86_64/lib/libmysqlclient.18.dylib /usr/lib/libmysqlclient.18.dylib 但是会出现 Operation Not Permitted，这个是 OSX 系统对系统文件的保护，在这链接找到解决方法：http://stackoverflow.com/questions/32659348/operation-not-permitted-when-on-root-el-capitan-rootless-disabled 言归正传，使用 pt-duplicate-key-checker -h localhost -p &#39;root&#39; -uroot，输出如下 # ######################################################################## # kaoqin.t_blob # ######################################################################## # idx_c_oid is a duplicate of PRIMARY # Key definitions: # KEY `idx_c_oid` (`id`), # PRIMARY KEY (`id`), # Column types: # `id` int(11) not null auto_increment # To remove this duplicate index, execute: ALTER TABLE `kaoqin`.`t_blob` DROP INDEX `idx_c_oid`; # idx_c_oid_2 is a duplicate of PRIMARY # Key definitions: # KEY `idx_c_oid_2` (`id`) # PRIMARY KEY (`id`), # Column types: # `id` int(11) not null auto_increment # To remove this duplicate index, execute: ALTER TABLE `kaoqin`.`t_blob` DROP INDEX `idx_c_oid_2`; # ######################################################################## # Summary of indexes # ######################################################################## # Size Duplicate Indexes 2464 # Total Duplicate Indexes 2 # Total Indexes 102 通过上述输出可以直接看出哪些是重复的索引，并且输出了删除索引的语句。 ##数据库结构优化 ###使用合适的数据类型 使用可以存在数据的最小数据类型； 使用简单的数据类型，比如 INT 比 varchar 上处理简单； 在 mysql 里尽可能使用 not null 定义字段； 尽量减少 text 的使用，非用不可的时候考虑分表； 日期：可以考虑使用 int 类型存储，结合使用 unix_timestamp()和 from_unixtime();select unix_timestamp(&#39;2016-06-06 12:11:11&#39;); 输出 1465186271select from_unixtime(1465186271); 输出 2016-06-06 12:11:11 IP 地址：使用 BIGINT 存储，结合使用INET_ATON()和INET_NTOA(); select inet_aton(&#39;192.168.0.1&#39;); 3232235521 select inet_ntoa(3232235521); 192.168.0.1 ###范式化和反范式化表结构设计一般情况下满足第三范式，在符合第三范式的表中查询 sql 需要关联很多个表，性能下降，这时候在表设计时候就需要允许数据允余来降低关联关系，达到提高性能的目的。 ###拆分垂直拆分：把原来一个有很多列的表拆分成多个表，解决了表宽度过宽的问题。通常遵循以下原则： 把不常用的字段放到一个表中； 把大字段独立放到一个表中； 把经常一起使用的字段放一起； 水平拆分：把原来一个行数很多的表水平拆分成多个表结构相同的表，解决了表行数过多的问题。常用拆分的手段,mod(id,5)值=0 存入 table_0值=2 存入 table_1值=3 存入 table_2值=4 存入 table_3值=5 存入 table_4 ##系统配置优化 ###优化 Mysql 的系统环境 Linux 网络方面配置，修改/etc/sysctl.conf #增加 tcp 队列数 net.ipv4.tcp_max_syn_backlog=65535 #减少断开连接时，资源回收 net.ipv4.tcp_max_tw_buckets=8000 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_fin_timeout=10 除此之外，最好是关闭系统上的 iptables、selinux 等软件防火墙，采用硬件防火墙。 修改打开文件的限制，/etc/security/limits.conf，增加以下内容 soft nofile 65535 hard nofile 65535###优化 Mysql 的配置文件Mysql 启动时候可以设置启动文件，Linux 中配置文件一般在/etc/my.cnf 或者 /etc/mysql/my.cnf,也可以使用命令查询：/usr/sbin/mysqld --verbose --help |grep -A 1 &#39;Default options&#39;使用 innodb 部分参数说明：innodb_buffer_pool_size用户配置 innodb 缓冲池大小，如果单机只运行 innodb 数据库，建议设置为内存的75%；innodb_buffer_pool_instancesmysql5.5后引入，可以设置 innodb 缓冲池的个数，均分innodb_buffer_pool_size，增加sql 语句执行的并发性；innodb_log_buffer_sizeinnodb 默认每秒钟把日志从 buffer 刷到磁盘，所以这个参数只需要保存每秒的日志就足够，不需要太大；innodb_flush_log_at_trx_commit关键的参数，影响 IO 效率，有三个可选择：0、1、2，默认为1，0：每次提交不刷新到磁盘，而是每秒把值刷新到磁盘；1：每次提交直接把值提交到磁盘；2：每次提交都是到 buffer 里，每秒缓冲区再把值提交到磁盘；所「1」是最安全的设置，考虑到 IO 性能的话，建议使用「2」。 innodb_read_io_threads 和 innodb_write_io_threadsinnodb 的读写并发线程数，可以结合系统的 Cpu 数量设置，也可以考虑到 Mysql 的读或者写负载来设置；innodb_file_per_table控制 innodb 的每一个表使用同一个共享表空间，默认为「OFF」,设置为「ON」后，每一个表使用独立的表空间，好处如下： 每个表使用独立表空间，可以增加 IO 读取的效率，因为同一个共享表空间的 IO 是顺序资源； 对于删除表或者 truncate 操作时候，表空间能迅速回收，而使用共享表空间的时候不可以；所以建议「ON」。innodb_stats_on_metadata决定了 innodb 在什么情况下刷新表的统计信息 ###使用工具优化配置参数可以借助 percona 的工具来生成配置文件：https://tools.percona.com/wizard最后生成一个初始化配置文件。 ##硬件优化略]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
</search>
